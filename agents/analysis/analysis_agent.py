"""
Analysis Agent - ë¶„ì„ ì¡°ìœ¨ ì—ì´ì „íŠ¸ (Orchestrator)

ì—­í• :
- ê°€ì„¤ ìƒì„± â†’ ê°€ì„¤ ê²€ì¦ (SQL) â†’ ì´ë²¤íŠ¸ ë§¤ì¹­ í”Œë¡œìš° ì¡°ìœ¨
- í•˜ìœ„ ì—ì´ì „íŠ¸ë“¤ì˜ í˜‘ì—… ê´€ë¦¬
- ìµœì¢… ë¶„ì„ ê²°ê³¼ ì¢…í•© (SQL ì¿¼ë¦¬ + ë§¤ì¹­ëœ ì´ë²¤íŠ¸ í¬í•¨)
"""

from typing import Dict, Any, List, Optional
from dataclasses import dataclass, field

from ..base import BaseAgent, AgentContext
from .hypothesis_generator import HypothesisGenerator, Hypothesis
from .hypothesis_validator import HypothesisValidator
from .event_matcher import EventMatcher, MatchedEvent


@dataclass
class KPIChange:
    """KPI ë³€ë™ ì •ë³´"""
    kpi_name: str  # ë§¤ì¶œ, ì›ê°€, íŒë§¤ìˆ˜ëŸ‰
    previous_value: float
    current_value: float
    change_percent: float
    change_amount: float
    period_info: str  # "2024 Q4 vs 2023 Q4"
    region: str = ""
    sql_query: str = ""


@dataclass
class AnalysisResult:
    """ë¶„ì„ ê²°ê³¼"""
    question: str
    kpi_change: KPIChange = None  # KPI ë³€ë™ (ë¨¼ì € ë³´ì—¬ì¤Œ)
    hypotheses: List[Hypothesis] = field(default_factory=list)
    validated_hypotheses: List[Hypothesis] = field(default_factory=list)
    matched_events: Dict[str, List[MatchedEvent]] = field(default_factory=dict)
    sql_queries: List[Dict] = field(default_factory=list)
    summary: str = ""
    sources: List[Dict] = field(default_factory=list)
    details: List[Dict] = field(default_factory=list)


REASONING_PROMPT = """ë‹¹ì‹ ì€ LGì „ì HE(Home Entertainment) ì‚¬ì—…ë¶€ì˜ ê²½ì˜ ì „ëµ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ê²½ì˜ì§„ì—ê²Œ ë³´ê³ í•  **í•µì‹¬ ì›ì¸ {top_k}ê°€ì§€**ë¥¼ ë¶„ì„í•˜ì„¸ìš”.

## ë¶„ì„ ì§ˆë¬¸
{question}

## KPI ë³€ë™ í˜„í™©
{kpi_summary}

## ë¶„ì„ ë°ì´í„°
{validated_hypotheses_detail}

---

## ì‘ì„± ì§€ì¹¨ (ê²½ì˜ì§„ ë³´ê³ ì„œ ìŠ¤íƒ€ì¼)

### 1. ë¬¸ì²´
- ê²½ì˜ ì „ëµíŒ€ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” **ë¹„ì¦ˆë‹ˆìŠ¤ ì–¸ì–´** ì‚¬ìš©
- ê¸°ìˆ ì  ìš©ì–´ (Factor, Score, Graph, INCREASES ë“±) **ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€**
- ìì—°ìŠ¤ëŸ½ê³  ë…¼ë¦¬ì ì¸ ë¬¸ì¥ìœ¼ë¡œ ì„œìˆ 

### 2. êµ¬ì¡°: ê° ì›ì¸ë³„ ì‹¬ì¸µ ë¶„ì„
ê° ì›ì¸ì— ëŒ€í•´ **ê²€ì¦ ìœ í˜•ì— ë”°ë¼** ë‹¤ë¥´ê²Œ ì„¤ëª…:

#### [ERP ë°ì´í„° ê²€ì¦ëœ ì›ì¸] (ì‹¤ì  ë°ì´í„° ê¸°ë°˜)
**ë°ì´í„° ë¶„ì„ ê²°ê³¼**:
- êµ¬ì²´ì  ìˆ˜ì¹˜ ë³€í™” (ì˜ˆ: "ë¬¼ë¥˜ë¹„ê°€ ì „ë…„ ëŒ€ë¹„ 15% ì¦ê°€í•˜ì—¬ ì›ê°€ ìƒìŠ¹")
- ì´ ë³€í™”ê°€ KPIì— ë¯¸ì¹œ ì •ëŸ‰ì  ì˜í–¥

**ì‹œì¥ í™˜ê²½ ìš”ì¸**: (ê´€ë ¨ ì´ë²¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°)
- ê´€ë ¨ ì‹œì¥ ë™í–¥, ì¶œì²˜ ì¸ìš© [1], [2] í˜•ì‹

#### [Knowledge Graph ê¸°ë°˜ ì›ì¸] (ì™¸ë¶€ ìš”ì¸, ERP ë°ì´í„° ì—†ìŒ)
**ì¸ê³¼ê´€ê³„ ë¶„ì„**:
- ì œê³µëœ ì¸ê³¼ê´€ê³„ ê²½ë¡œë¥¼ ìì—°ì–´ë¡œ ì„¤ëª…
- ì˜ˆ: "í™í•´ ì‚¬íƒœë¡œ ì¸í•œ í•´ìƒìš´ì„ ìƒìŠ¹ì´ ë¬¼ë¥˜ë¹„ ì¦ê°€ë¡œ ì´ì–´ì ¸ ì›ê°€ ìƒìŠ¹ ì••ë ¥"
- **ì£¼ì˜**: êµ¬ì²´ì  ìˆ˜ì¹˜ ë³€í™”ëŠ” ì–¸ê¸‰í•˜ì§€ ë§ ê²ƒ (ERPì— í•´ë‹¹ ë°ì´í„° ì—†ìŒ)

**ì‹œì¥ í™˜ê²½ ìš”ì¸**:
- ê´€ë ¨ ì‹œì¥ ë™í–¥, ì¶œì²˜ ì¸ìš© [1], [2] í˜•ì‹

### 3. ì‚¬ì—… ì˜í–¥ (ê·¼ê±°ê°€ ìˆëŠ” ê²½ìš°ë§Œ)
- ERP ê²€ì¦: ìˆ˜ì¹˜ ë³€í™”ê°€ ì „ì²´ KPIì—ì„œ ì°¨ì§€í•˜ëŠ” ë¹„ì¤‘ìœ¼ë¡œ ì˜í–¥ ì„¤ëª…
- Graph ê²€ì¦: ì¸ê³¼ê´€ê³„ ê²½ë¡œì—ì„œ ë„ì¶œëœ ì˜í–¥ë§Œ ì„¤ëª…
- **ê·¼ê±° ì—†ì´ ì¶”ì¸¡í•˜ì§€ ë§ ê²ƒ**

### 4. ë¶„ëŸ‰
- ê° ì›ì¸ë‹¹ **150-250ì** ìƒì„¸ ì„¤ëª…
- ì´ ë¶„ì„ ë¶„ëŸ‰: 600-900ì

### 5. ì •í™•ì„±
- ì œê³µëœ ë°ì´í„°ì™€ ë‰´ìŠ¤ë§Œ ì¸ìš© (ìƒˆë¡œìš´ ìˆ˜ì¹˜ ìƒì„± ê¸ˆì§€)
- Graph ê¸°ë°˜ ì›ì¸ì€ "~ë¡œ ë¶„ì„ë¨", "~ì— ê¸°ì¸í•œ ê²ƒìœ¼ë¡œ íŒë‹¨ë¨" ë“±ìœ¼ë¡œ í‘œí˜„
- ERPì— ì—†ëŠ” ì™¸ë¶€ ìš”ì¸(í™˜ìœ¨, ê²½ìŸ, ìˆ˜ìš” ë“±)ì€ ìˆ˜ì¹˜ ë³€í™”ë¥¼ ì–¸ê¸‰í•˜ì§€ ì•ŠìŒ

### 6. ê²°ë¡ 
ë§ˆì§€ë§‰ì— **ì¢…í•© ë¶„ì„** (2-3ë¬¸ì¥):
- í•µì‹¬ ì›ì¸ë“¤ì˜ ë³µí•© ì‘ìš©
- ê²½ì˜ ì „ëµì  ì‹œì‚¬ì 

## ì‘ë‹µ
"""


class AnalysisAgent(BaseAgent):
    """ë¶„ì„ ì¡°ìœ¨ ì—ì´ì „íŠ¸"""

    name = "analysis_agent"
    description = "ê°€ì„¤ ìƒì„±, SQL ê²€ì¦, ì´ë²¤íŠ¸ ë§¤ì¹­ì„ ì¡°ìœ¨í•˜ì—¬ KPI ë³€ë™ ì›ì¸ì„ ë¶„ì„í•©ë‹ˆë‹¤."

    # KPI ì¶”ì¶œ íŒ¨í„´ (ì‹¤ì œ DB ìŠ¤í‚¤ë§ˆì— ë§ì¶¤)
    KPI_PATTERNS = {
        "ë§¤ì¶œ": {
            "keywords": ["ë§¤ì¶œ", "revenue", "sales", "ìˆ˜ìµ"],
            "query_template": """
                SELECT
                    CASE
                        WHEN DOC_DATE >= '{prev_start}' AND DOC_DATE <= '{prev_end}' THEN 'Previous'
                        WHEN DOC_DATE >= '{curr_start}' AND DOC_DATE <= '{curr_end}' THEN 'Current'
                    END AS PERIOD,
                    SUM(si.NET_VALUE) AS TOTAL_VALUE
                FROM TBL_TX_SALES_HEADER sh
                JOIN TBL_TX_SALES_ITEM si ON sh.ORDER_NO = si.ORDER_NO
                WHERE (
                    (DOC_DATE >= '{prev_start}' AND DOC_DATE <= '{prev_end}')
                    OR (DOC_DATE >= '{curr_start}' AND DOC_DATE <= '{curr_end}')
                ) {region_filter}
                GROUP BY PERIOD
            """
        },
        "ì›ê°€": {
            "keywords": ["ì›ê°€", "cost", "ë¹„ìš©"],
            "query_template": """
                SELECT
                    CASE
                        WHEN sh.DOC_DATE >= '{prev_start}' AND sh.DOC_DATE <= '{prev_end}' THEN 'Previous'
                        WHEN sh.DOC_DATE >= '{curr_start}' AND sh.DOC_DATE <= '{curr_end}' THEN 'Current'
                    END AS PERIOD,
                    SUM(cd.COST_AMOUNT) AS TOTAL_VALUE
                FROM TBL_TX_SALES_HEADER sh
                JOIN TBL_TX_COST_DETAIL cd ON sh.ORDER_NO = cd.ORDER_NO
                WHERE (
                    (sh.DOC_DATE >= '{prev_start}' AND sh.DOC_DATE <= '{prev_end}')
                    OR (sh.DOC_DATE >= '{curr_start}' AND sh.DOC_DATE <= '{curr_end}')
                ) {region_filter}
                GROUP BY PERIOD
            """
        },
        "íŒë§¤ìˆ˜ëŸ‰": {
            "keywords": ["íŒë§¤ëŸ‰", "ìˆ˜ëŸ‰", "quantity", "volume"],
            "query_template": """
                SELECT
                    CASE
                        WHEN DOC_DATE >= '{prev_start}' AND DOC_DATE <= '{prev_end}' THEN 'Previous'
                        WHEN DOC_DATE >= '{curr_start}' AND DOC_DATE <= '{curr_end}' THEN 'Current'
                    END AS PERIOD,
                    SUM(si.ORDER_QTY) AS TOTAL_VALUE
                FROM TBL_TX_SALES_HEADER sh
                JOIN TBL_TX_SALES_ITEM si ON sh.ORDER_NO = si.ORDER_NO
                WHERE (
                    (DOC_DATE >= '{prev_start}' AND DOC_DATE <= '{prev_end}')
                    OR (DOC_DATE >= '{curr_start}' AND DOC_DATE <= '{curr_end}')
                ) {region_filter}
                GROUP BY PERIOD
            """
        }
    }

    # ì§€ì—­ â†’ Subsidiary ë§¤í•‘
    REGION_SUBSIDIARY_MAP = {
        "NA": ["LGEUS", "LGECA"],
        "EU": ["LGEDE", "LGEFR", "LGEUK"],
        "KR": ["LGEKR"],
        "US": ["LGEUS"],
        "ë¶ë¯¸": ["LGEUS", "LGECA"],
        "ìœ ëŸ½": ["LGEDE", "LGEFR", "LGEUK"],
        "í•œêµ­": ["LGEKR"]
    }

    # ìœ ì‚¬ Factor ê·¸ë£¹í™” (ëŒ€í‘œ Factor â†’ ìœ ì‚¬ Factor ëª©ë¡)
    FACTOR_GROUPS = {
        # ìˆ˜ìš” ê´€ë ¨
        "ìˆ˜ìš” ë³€ë™": ["ìˆ˜ìš”", "ê¸€ë¡œë²Œìˆ˜ìš”", "ì§€ì—­ë³„ìˆ˜ìš”", "ê³„ì ˆì  ìˆ˜ìš”", "ê³„ì ˆì ìˆ˜ìš”", "IT ì„¸íŠ¸ ìˆ˜ìš” ë‘”í™”",
                    "ìˆ˜ìš”ë¶€ì§„", "ìˆ˜ìš” ë¶€ì§„", "TVìˆ˜ìš”", "ê°€ì „ìˆ˜ìš”", "ì„±ìˆ˜ê¸°íš¨ê³¼", "ì„±ìˆ˜ê¸° íš¨ê³¼"],
        # ê²½ê¸°/ì†Œë¹„ ê´€ë ¨
        "ê²½ê¸°/ì†Œë¹„ì‹¬ë¦¬": ["ê²½ê¸°ë¶€ì§„", "ê²½ê¸° ë¶€ì§„", "ì†Œë¹„ì‹¬ë¦¬ìœ„ì¶•", "ì†Œë¹„ì‹¬ë¦¬ ìœ„ì¶•", "ì†Œë¹„ ì‹¬ë¦¬",
                      "ì¹¨ì²´ëœ ì£¼íƒ ë§¤ë§¤", "ì£¼íƒ ë§¤ë§¤", "ê²½ê¸°ì¹¨ì²´", "ì†Œë¹„ ë‘”í™”"],
        # í™˜ìœ¨ ê´€ë ¨
        "í™˜ìœ¨": ["í™˜ìœ¨", "ì›/ë‹¬ëŸ¬ í™˜ìœ¨", "ë‹¬ëŸ¬ í™˜ìœ¨", "ì›ë‹¬ëŸ¬", "ë‹¬ëŸ¬ ê°•ì„¸"],
        # ê²½ìŸ ê´€ë ¨
        "ê²½ìŸ ì‹¬í™”": ["ê²½ìŸì‹¬í™”", "ê²½ìŸ ì‹¬í™”", "ê°€ê²©ê²½ìŸ", "ì¤‘êµ­ì—…ì²´ ê²½ìŸ", "TCL", "í•˜ì´ì„¼ìŠ¤"],
        # ë¬¼ë¥˜/ìš´ì„ ê´€ë ¨
        "ë¬¼ë¥˜ë¹„/ìš´ì„": ["ë¬¼ë¥˜ë¹„", "í•´ìƒìš´ì„", "ìš´ì„", "ì»¨í…Œì´ë„ˆ ìš´ì„", "í™í•´ ì‚¬íƒœ"],
        # íŒ¨ë„/ë¶€í’ˆ ê´€ë ¨
        "íŒ¨ë„/ë¶€í’ˆ ê°€ê²©": ["íŒ¨ë„ê°€ê²©", "íŒ¨ë„ ê°€ê²©", "ë””ìŠ¤í”Œë ˆì´ ê°€ê²©", "OLED íŒ¨ë„", "LCD íŒ¨ë„", "ë¶€í’ˆë¹„"],
        # ê´€ì„¸ ê´€ë ¨
        "ê´€ì„¸/ë¬´ì—­": ["ê´€ì„¸", "ê´€ì„¸ìœ¨", "ìˆ˜ì…ê´€ì„¸", "íŠ¸ëŸ¼í”„ ê´€ì„¸", "ë¬´ì—­ë¶„ìŸ"],
    }

    # ë¶„ì„ ì„¤ì •
    TOP_K_FACTORS = 3  # ìƒìœ„ ëª‡ ê°œ ì›ì¸ë§Œ ìƒì„¸ ë¶„ì„
    MIN_EVENT_SCORE = 0.5  # ì´ë²¤íŠ¸ ìµœì†Œ ë§¤ì¹­ ì ìˆ˜
    REASONING_MODEL = "gpt-4o"  # ì¶”ë¡  ëª¨ë¸: o1, o1-mini, gpt-4o (o1 ë¯¸ì§€ì› ì‹œ gpt-4o ì‚¬ìš©)

    def __init__(self, api_key: str = None, db_path: str = None):
        super().__init__(api_key)
        self.db_path = db_path

        # í•˜ìœ„ ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        self.hypothesis_generator = HypothesisGenerator(api_key)
        self.hypothesis_validator = HypothesisValidator(api_key, db_path)
        self.event_matcher = EventMatcher(api_key)

        self.add_sub_agent(self.hypothesis_generator)
        self.add_sub_agent(self.hypothesis_validator)
        self.add_sub_agent(self.event_matcher)

    def analyze(
        self,
        question: str,
        period: Dict = None,
        region: str = None,
        company: str = "LGE",
        verbose: bool = True
    ) -> AnalysisResult:
        """
        KPI ë³€ë™ ì›ì¸ ë¶„ì„ ì‹¤í–‰

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            period: {"year": 2024, "quarter": 4}
            region: "NA", "EU", "KR" ë“±
            company: íšŒì‚¬ ì½”ë“œ
            verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
        """
        if verbose:
            print("=" * 60)
            print(f"ì§ˆë¬¸: {question}")
            print("=" * 60)

        # ê¸°ë³¸ ê¸°ê°„ ì„¤ì •
        if not period:
            period = {"year": 2024, "quarter": 4}

        # Step 0: KPI ë³€ë™ ê³„ì‚° (ë§¤ì¶œ/ì›ê°€/ìˆ˜ëŸ‰ ìì²´ì˜ ë³€ë™)
        if verbose:
            print("\n[Step 0] KPI ë³€ë™ ê³„ì‚° ì¤‘...")

        kpi_change = self._calculate_kpi_change(question, period, region)

        if verbose and kpi_change:
            print(f"  {kpi_change.kpi_name}: {kpi_change.previous_value:,.0f} â†’ {kpi_change.current_value:,.0f} ({kpi_change.change_percent:+.1f}%)")
            print(f"  ë¹„êµ ê¸°ê°„: {kpi_change.period_info}")

        # Step 1: ê°€ì„¤ ìƒì„±
        if verbose:
            print("\n[Step 1] ê°€ì„¤ ìƒì„± ì¤‘...")

        hypotheses = self.hypothesis_generator.generate(
            question=question,
            company=company,
            period=f"{period['year']}ë…„ Q{period['quarter']}",
            region=region
        )

        if verbose:
            print(f"  ìƒì„±ëœ ê°€ì„¤: {len(hypotheses)}ê°œ")
            for h in hypotheses:
                print(f"    - [{h.id}] {h.description}")

        # Step 2: ê°€ì„¤ ê²€ì¦ (SQL Agent)
        if verbose:
            print("\n[Step 2] ê°€ì„¤ ê²€ì¦ ì¤‘ (SQL Agent)...")

        validated = self.hypothesis_validator.validate(
            hypotheses=hypotheses,
            period=period,
            region=region,
            threshold=5.0
        )

        # SQL ì¿¼ë¦¬ ìˆ˜ì§‘
        sql_queries = []
        if verbose:
            print(f"  ê²€ì¦ëœ ê°€ì„¤: {len(validated)}ê°œ")
            for h in validated:
                data = h.validation_data or {}
                print(f"    - [{h.id}] {h.factor}: {data.get('details', '')}")

                # SQL ì¿¼ë¦¬ ì €ì¥ ë° ì¶œë ¥
                sql_query = data.get("sql_query", "")
                if sql_query:
                    sql_queries.append({
                        "hypothesis_id": h.id,
                        "factor": h.factor,
                        "sql": sql_query
                    })
                    print(f"      SQL: {sql_query[:100]}...")

        # Step 3: ì´ë²¤íŠ¸ ë§¤ì¹­ (Scoring Algorithm)
        if verbose:
            print("\n[Step 3] ì´ë²¤íŠ¸ ë§¤ì¹­ ì¤‘ (Scoring Algorithm)...")

        matched_events = {}
        try:
            matched_events = self.event_matcher.match(
                hypotheses=validated,
                region=region,
                min_score=0.3,  # 0-1 ìŠ¤ì¼€ì¼
                top_k=5
            )

            if verbose:
                for h_id, events in matched_events.items():
                    print(f"  [{h_id}] ë§¤ì¹­ëœ ì´ë²¤íŠ¸: {len(events)}ê°œ")
                    for ev in events[:3]:
                        print(f"    - {ev.event_name} (Score: {ev.total_score:.1f})")
                        if ev.sources:
                            print(f"      ì¶œì²˜: {ev.sources[0].get('title', '')[:50]}...")

        except Exception as e:
            if verbose:
                print(f"  ì´ë²¤íŠ¸ ë§¤ì¹­ ì˜¤ë¥˜: {e}")

        # Step 4: ê²°ê³¼ ì¢…í•©
        if verbose:
            print("\n[Step 4] ê²°ê³¼ ì¢…í•© ì¤‘...")

        result = AnalysisResult(
            question=question,
            kpi_change=kpi_change,  # KPI ë³€ë™ ì •ë³´ ì¶”ê°€
            hypotheses=hypotheses,
            validated_hypotheses=validated,
            matched_events=matched_events,
            sql_queries=sql_queries
        )

        # ìƒì„¸ ë¶„ì„ ê²°ê³¼ êµ¬ì„±
        result.details = self._build_details(validated, matched_events, sql_queries)

        # Step 5: ì¶”ë¡  ê¸°ë°˜ ìš”ì•½ ìƒì„± (ì¶œì²˜ í¬í•¨)
        if verbose:
            print("\n[Step 5] ì¶”ë¡  ê¸°ë°˜ ë‹µë³€ ìƒì„± ì¤‘...")

        summary_result = self._generate_summary(question, result.details, kpi_change)
        result.summary = summary_result["summary"]
        result.sources = summary_result["sources"]

        if verbose:
            print(f"  ì¶œì²˜ ìˆ˜: {len(result.sources)}ê°œ")
            print("\n" + "=" * 60)
            print("ë¶„ì„ ì™„ë£Œ!")
            print("=" * 60)

        return result

    def _build_details(
        self,
        validated: List[Hypothesis],
        matched_events: Dict[str, List[MatchedEvent]],
        sql_queries: List[Dict]
    ) -> List[Dict]:
        """ìƒì„¸ ë¶„ì„ ê²°ê³¼ êµ¬ì„± (SQL/Graph ê²€ì¦ íƒ€ì… êµ¬ë¶„)"""
        details = []

        # SQL ì¿¼ë¦¬ë¥¼ hypothesis_idë¡œ ë§¤í•‘
        sql_map = {q["hypothesis_id"]: q["sql"] for q in sql_queries}

        for hypothesis in validated:
            h_data = hypothesis.validation_data or {}

            # ê²€ì¦ íƒ€ì… í™•ì¸ (sql ë˜ëŠ” graph)
            validation_type = h_data.get("validation_type", "sql")
            graph_evidence = h_data.get("graph_evidence", {})

            prev_val = h_data.get("previous_value", 0)
            curr_val = h_data.get("current_value", 0)
            change_pct = h_data.get("change_percent", 0)

            # ë°ì´í„° ë°©í–¥ì„± í•´ì„ (SQL ê²€ì¦ëœ ê²½ìš°ë§Œ)
            if validation_type == "sql" and (prev_val != 0 or curr_val != 0):
                # ìŒìˆ˜ê°’: ë¹„ìš©/ì†ì‹¤ â†’ ê°’ì´ ì»¤ì§€ë©´(ëœ ìŒìˆ˜) ê°œì„ , ì‘ì•„ì§€ë©´(ë” ìŒìˆ˜) ì•…í™”
                # ì–‘ìˆ˜ê°’: ë§¤ì¶œ/ì´ìµ â†’ ê°’ì´ ì»¤ì§€ë©´ ê°œì„ , ì‘ì•„ì§€ë©´ ì•…í™”
                if prev_val < 0 and curr_val < 0:
                    if curr_val > prev_val:
                        interpretation = "ê°œì„  (ì†ì‹¤/ë¹„ìš© ê°ì†Œ)"
                        impact_direction = "positive"
                    else:
                        interpretation = "ì•…í™” (ì†ì‹¤/ë¹„ìš© ì¦ê°€)"
                        impact_direction = "negative"
                elif prev_val >= 0 and curr_val >= 0:
                    if curr_val > prev_val:
                        interpretation = "ì¦ê°€"
                        impact_direction = "positive"
                    else:
                        interpretation = "ê°ì†Œ"
                        impact_direction = "negative"
                else:
                    if curr_val > prev_val:
                        interpretation = "ê°œì„  (ì ìâ†’í‘ì ë˜ëŠ” ì†ì‹¤ ê°ì†Œ)"
                        impact_direction = "positive"
                    else:
                        interpretation = "ì•…í™” (í‘ìâ†’ì ì ë˜ëŠ” ì†ì‹¤ ì¦ê°€)"
                        impact_direction = "negative"
            else:
                # Graph ê²€ì¦ì¸ ê²½ìš°: ì¸ê³¼ê´€ê³„ ê²½ë¡œì—ì„œ í•´ì„
                interpretation = h_data.get("details", hypothesis.description)
                impact_direction = hypothesis.direction  # increase/decrease

            # ìƒì„¸ ê²°ê³¼ êµ¬ì„±
            detail = {
                "factor": hypothesis.factor,
                "category": hypothesis.category,
                "description": hypothesis.description,
                "validation_type": validation_type,  # "sql" or "graph"
                "change_percent": change_pct,
                "previous_value": prev_val,
                "current_value": curr_val,
                "direction": h_data.get("direction", hypothesis.direction),
                "interpretation": interpretation,
                "impact_direction": impact_direction,
                "sql_query": sql_map.get(hypothesis.id, "") if validation_type == "sql" else "",
                "matched_events": [],
                # Graph ê²€ì¦ ì‹œ ì¸ê³¼ê´€ê³„ ê²½ë¡œ í¬í•¨
                "graph_evidence": graph_evidence if validation_type == "graph" else {},
                "causal_chains": graph_evidence.get("causal_chains", []) if validation_type == "graph" else []
            }

            # ë§¤ì¹­ëœ ì´ë²¤íŠ¸ ì¶”ê°€ (Scoring Algorithm ê²°ê³¼)
            events = matched_events.get(hypothesis.id, [])
            for ev in events[:5]:
                detail["matched_events"].append({
                    "name": ev.event_name,
                    "category": ev.event_category,
                    "severity": ev.severity,
                    "impact": ev.impact_type,
                    "score": ev.total_score,
                    "score_breakdown": ev.score_breakdown,
                    "sources": ev.sources[:2],
                    "evidence": ev.evidence[:200] if ev.evidence else ""
                })

            details.append(detail)

        # ì •ë ¬: SQL ê²€ì¦(ìˆ˜ì¹˜ ìˆìŒ)ì€ ë³€í™”ìœ¨ ìˆœ, Graph ê²€ì¦ì€ ì´ë²¤íŠ¸ ìˆ˜ ìˆœ
        def sort_key(d):
            if d["validation_type"] == "sql" and d["change_percent"] != 0:
                return (0, abs(d["change_percent"]))  # SQL ê²€ì¦ ìš°ì„ , ë³€í™”ìœ¨ ìˆœ
            else:
                return (1, len(d.get("matched_events", [])))  # GraphëŠ” ì´ë²¤íŠ¸ ìˆ˜ ìˆœ

        details.sort(key=sort_key, reverse=True)

        return details

    def _get_representative_factor(self, factor_name: str) -> str:
        """Factorì˜ ëŒ€í‘œ ê·¸ë£¹ëª… ë°˜í™˜"""
        factor_lower = factor_name.lower().strip()
        for group_name, members in self.FACTOR_GROUPS.items():
            for member in members:
                if member.lower() in factor_lower or factor_lower in member.lower():
                    return group_name
        return factor_name  # ê·¸ë£¹ì— ì—†ìœ¼ë©´ ì›ë˜ ì´ë¦„ ë°˜í™˜

    def _select_top_factors(
        self,
        details: List[Dict],
        top_k: int = None
    ) -> List[Dict]:
        """
        ìœ ì‚¬ Factor ê·¸ë£¹í™” í›„ Top K ì„ ì •

        ì„ ì • ê¸°ì¤€:
        1. ê·¸ë£¹ë³„ ëŒ€í‘œ Factor ì„ ì • (ê°€ì¥ ë†’ì€ ë³€í™”ìœ¨)
        2. ì´ë²¤íŠ¸ ë§¤ì¹­ í’ˆì§ˆ (ê³ í’ˆì§ˆ ì´ë²¤íŠ¸ê°€ ìˆëŠ” Factor ìš°ì„ )
        3. ë³€í™”ìœ¨ í¬ê¸° ìˆœ ì •ë ¬
        """
        if top_k is None:
            top_k = self.TOP_K_FACTORS

        if not details:
            return []

        # 1. ê·¸ë£¹ë³„ë¡œ Factor ë¶„ë¥˜
        group_map = {}  # group_name -> [details]
        for d in details:
            factor = d["factor"]
            group = self._get_representative_factor(factor)
            if group not in group_map:
                group_map[group] = []
            group_map[group].append(d)

        # 2. ê° ê·¸ë£¹ì—ì„œ ëŒ€í‘œ Factor ì„ ì • (ë³€í™”ìœ¨ + ì´ë²¤íŠ¸ í’ˆì§ˆ)
        representatives = []
        for group_name, group_details in group_map.items():
            # ê·¸ë£¹ ë‚´ ì •ë ¬: ì´ë²¤íŠ¸ í’ˆì§ˆ â†’ ë³€í™”ìœ¨
            def score_detail(d):
                change_score = abs(d["change_percent"])
                # ê³ í’ˆì§ˆ ì´ë²¤íŠ¸ ë³´ë„ˆìŠ¤ (score >= MIN_EVENT_SCORE)
                high_quality_events = [
                    e for e in d.get("matched_events", [])
                    if e.get("score", 0) >= self.MIN_EVENT_SCORE
                ]
                event_bonus = len(high_quality_events) * 10
                return change_score + event_bonus

            group_details.sort(key=score_detail, reverse=True)
            best = group_details[0]

            # ê·¸ë£¹ ì •ë³´ ì¶”ê°€
            best["group_name"] = group_name
            best["group_size"] = len(group_details)
            if len(group_details) > 1:
                best["related_factors"] = [d["factor"] for d in group_details[1:]]
            else:
                best["related_factors"] = []

            representatives.append(best)

        # 3. ëŒ€í‘œ Factorë“¤ ì¤‘ Top K ì„ ì •
        def final_score(d):
            change_score = abs(d["change_percent"])
            high_quality_events = [
                e for e in d.get("matched_events", [])
                if e.get("score", 0) >= self.MIN_EVENT_SCORE
            ]
            event_bonus = len(high_quality_events) * 15
            return change_score + event_bonus

        representatives.sort(key=final_score, reverse=True)

        return representatives[:top_k]

    def _generate_summary(
        self,
        question: str,
        details: List[Dict],
        kpi_change: KPIChange = None
    ) -> Dict[str, Any]:
        """ì¶”ë¡  ëª¨ë¸ ê¸°ë°˜ ë¶„ì„ ìš”ì•½ ìƒì„± (Top K í•µì‹¬ ì›ì¸ ì‹¬ì¸µ ë¶„ì„)"""
        if not details and not kpi_change:
            return {
                "summary": "ê²€ì¦ëœ ì›ì¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                "sources": []
            }

        # 0. KPI ë³€ë™ í˜„í™© í¬ë§·íŒ…
        if kpi_change:
            change_direction = "ì¦ê°€" if kpi_change.change_percent > 0 else "ê°ì†Œ"
            kpi_summary = f"""**{kpi_change.kpi_name}** ë³€ë™:
- ê¸°ê°„: {kpi_change.period_info}
- ì´ì „ ê¸°ê°„: {kpi_change.previous_value:,.0f}
- í˜„ì¬ ê¸°ê°„: {kpi_change.current_value:,.0f}
- ë³€í™”ìœ¨: **{kpi_change.change_percent:+.1f}%** ({change_direction})
- ë³€í™” ê¸ˆì•¡: {kpi_change.change_amount:+,.0f}
"""
        else:
            kpi_summary = "(KPI ë³€ë™ ì •ë³´ ì—†ìŒ)"

        # 1. Top K Factor ì„ ì • (ìœ ì‚¬ Factor ê·¸ë£¹í™” í›„)
        print(f"[AnalysisAgent] ì „ì²´ ê²€ì¦ëœ ê°€ì„¤: {len(details)}ê°œ")
        top_factors = self._select_top_factors(details, self.TOP_K_FACTORS)
        top_k = len(top_factors)
        print(f"[AnalysisAgent] Top {self.TOP_K_FACTORS} ì„ ì • ê²°ê³¼: {top_k}ê°œ")

        # Top Factorê°€ ì—†ìœ¼ë©´ ì›ë³¸ details ì‚¬ìš© (ìµœëŒ€ 3ê°œ)
        if not top_factors and details:
            print("[AnalysisAgent] Top Factor ì„ ì • ì‹¤íŒ¨, ì›ë³¸ ë°ì´í„° ì‚¬ìš©")
            top_factors = details[:self.TOP_K_FACTORS]
            top_k = len(top_factors)

        # 2. ì„ ì •ëœ Factorë³„ ìƒì„¸ ì •ë³´ êµ¬ì„±
        all_sources = []
        source_idx = 1
        validated_hypotheses_detail = ""

        if top_factors:
            for i, d in enumerate(top_factors, 1):
                factor = d['factor']
                category = d['category']
                change_pct = d['change_percent']
                prev_val = d['previous_value']
                curr_val = d['current_value']
                interpretation = d.get('interpretation', d.get('direction', ''))
                validation_type = d.get('validation_type', 'sql')
                causal_chains = d.get('causal_chains', [])

                # ê·¸ë£¹ ì •ë³´
                group_name = d.get('group_name', factor)

                # ì¹´í…Œê³ ë¦¬ í•œê¸€í™”
                category_kr = {
                    "cost": "ì›ê°€ ìš”ì¸",
                    "revenue": "ë§¤ì¶œ ìš”ì¸",
                    "pricing": "ê°€ê²© ìš”ì¸",
                    "external": "ì™¸ë¶€ í™˜ê²½"
                }.get(category, category)

                # ê²€ì¦ íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥¸ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
                if validation_type == "sql" and (prev_val != 0 or curr_val != 0):
                    # SQL ê²€ì¦: ì‹¤ì  ë°ì´í„° ê¸°ë°˜
                    validated_hypotheses_detail += f"""
### ì›ì¸ {i}: {group_name}
**ë¶„ë¥˜:** {category_kr}
**ê²€ì¦ ë°©ì‹:** ERP ì‹¤ì  ë°ì´í„°

**ì‹¤ì  ë°ì´í„° ë³€í™”:**
- ë³€í™”ìœ¨: {change_pct:+.1f}%
- ì „ë…„ ë™ê¸°: {prev_val:,.0f}
- ë‹¹ê¸°: {curr_val:,.0f}
- í•´ì„: {interpretation}
"""
                else:
                    # Graph ê²€ì¦: ì¸ê³¼ê´€ê³„ ê²½ë¡œ ê¸°ë°˜
                    validated_hypotheses_detail += f"""
### ì›ì¸ {i}: {group_name}
**ë¶„ë¥˜:** {category_kr}
**ê²€ì¦ ë°©ì‹:** Knowledge Graph ì¸ê³¼ê´€ê³„ ë¶„ì„ (ERPì— í•´ë‹¹ ë°ì´í„° ì—†ìŒ)

**ì¸ê³¼ê´€ê³„ ê²½ë¡œ:**
"""
                    # ì¸ê³¼ê´€ê³„ ê²½ë¡œ ì¶œë ¥
                    if causal_chains:
                        for chain in causal_chains[:3]:
                            chain_text = chain.get('chain_text', '')
                            if chain_text:
                                validated_hypotheses_detail += f"- {chain_text}\n"
                    else:
                        validated_hypotheses_detail += f"- {interpretation}\n"

                    validated_hypotheses_detail += """
**ì£¼ì˜:** ì´ ìš”ì¸ì€ ERPì— ì§ì ‘ì ì¸ ìˆ˜ì¹˜ ë°ì´í„°ê°€ ì—†ì–´ ì •ëŸ‰ì  ì˜í–¥ì„ ì‚°ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
ì•„ë˜ ì‹œì¥ ë™í–¥ì„ ë°”íƒ•ìœ¼ë¡œ ì •ì„±ì  ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

                # ì™¸ë¶€ ì´ë²¤íŠ¸ ì¶”ê°€ (ê³ í’ˆì§ˆ ì´ë²¤íŠ¸ë§Œ, ë¹„ì¦ˆë‹ˆìŠ¤ ì–¸ì–´ë¡œ)
                matched_events = d.get('matched_events', [])
                high_quality_events = [
                    e for e in matched_events
                    if e.get('score', 0) >= self.MIN_EVENT_SCORE
                ]

                if high_quality_events:
                    validated_hypotheses_detail += f"\n**ê´€ë ¨ ì‹œì¥ ë™í–¥:**\n"

                    for ev in high_quality_events[:3]:
                        event_name = ev.get('name', '')
                        evidence = ev.get('evidence', '')

                        # ì¶œì²˜ ìˆ˜ì§‘
                        event_source_refs = []
                        for src in ev.get('sources', [])[:2]:
                            title = src.get('title', 'ì œëª© ì—†ìŒ')
                            url = src.get('link', src.get('url', ''))
                            if url:
                                existing = next((s for s in all_sources if s['url'] == url), None)
                                if existing:
                                    event_source_refs.append(f"[{existing['idx']}]")
                                else:
                                    all_sources.append({
                                        "idx": source_idx,
                                        "title": title,
                                        "url": url,
                                        "event": event_name,
                                        "factor": factor
                                    })
                                    event_source_refs.append(f"[{source_idx}]")
                                    source_idx += 1

                        source_str = " ".join(event_source_refs)

                        # ë¹„ì¦ˆë‹ˆìŠ¤ ì¹œí™”ì  í¬ë§· (ê¸°ìˆ  ìš©ì–´ ì œê±°)
                        validated_hypotheses_detail += f"""
- **{event_name}** {source_str}
  {evidence[:400] if evidence else ''}
"""
                else:
                    if validation_type == "sql":
                        validated_hypotheses_detail += "\n**ê´€ë ¨ ì‹œì¥ ë™í–¥:** ì§ì ‘ ê´€ë ¨ëœ ì™¸ë¶€ ì´ìŠˆê°€ í™•ì¸ë˜ì§€ ì•ŠìŒ (ë‚´ë¶€ ì‹¤ì  ë°ì´í„° ê¸°ë°˜ ë¶„ì„)\n"
                    else:
                        validated_hypotheses_detail += "\n**ê´€ë ¨ ì‹œì¥ ë™í–¥:** ê´€ë ¨ ë‰´ìŠ¤/ì´ë²¤íŠ¸ê°€ í™•ì¸ë˜ì§€ ì•ŠìŒ\n"

        else:
            validated_hypotheses_detail = "(ë¶„ì„ ê°€ëŠ¥í•œ ë°ì´í„° ì—†ìŒ)"

        # ì¶œì²˜ ëª©ë¡ ì¶”ê°€
        if all_sources:
            validated_hypotheses_detail += "\n---\n**ğŸ“š ì°¸ê³  ì¶œì²˜:**\n"
            for src in all_sources:
                validated_hypotheses_detail += f"[{src['idx']}] {src['title']}\n"

        # 3. ì¶”ë¡  í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = REASONING_PROMPT.format(
            question=question,
            kpi_summary=kpi_summary,
            validated_hypotheses_detail=validated_hypotheses_detail,
            top_k=top_k
        )

        # 4. ì¶”ë¡  ëª¨ë¸ í˜¸ì¶œ (o1 â†’ gpt-4o fallback)
        system_prompt = """ë‹¹ì‹ ì€ LGì „ì ê²½ì˜ ì „ëµì‹¤ ì†Œì† ì¬ë¬´/ì‹œì¥ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‘ì„± ì›ì¹™:
1. ê²½ì˜ì§„ì´ ë°”ë¡œ ì´í•´í•  ìˆ˜ ìˆëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ì–¸ì–´ë§Œ ì‚¬ìš©
2. ê¸°ìˆ  ìš©ì–´ ì ˆëŒ€ ê¸ˆì§€: Factor, Score, Graph, INCREASES, DECREASES, KPI ë“±
3. ìì—°ìŠ¤ëŸ½ê³  ë…¼ë¦¬ì ì¸ ë¬¸ì¥ìœ¼ë¡œ íë¦„ìˆê²Œ ì„œìˆ 
4. ì‹¤ì  ìˆ˜ì¹˜ëŠ” ì •í™•íˆ ì¸ìš©í•˜ë˜, "ì „ë…„ ëŒ€ë¹„ 24% ê°ì†Œ" ë“± ìì—°ì–´ë¡œ í‘œí˜„
5. ì‹œì¥ ë™í–¥ì€ êµ¬ì²´ì  ì‚¬ë¡€ì™€ ì¶œì²˜ ë²ˆí˜¸ [1], [2]ë¡œ ì¸ìš©"""

        summary = None

        # 1ì°¨ ì‹œë„: ì„¤ì •ëœ ì¶”ë¡  ëª¨ë¸
        try:
            print(f"[AnalysisAgent] ì¶”ë¡  ëª¨ë¸ í˜¸ì¶œ: {self.REASONING_MODEL}")
            summary = self._call_llm(
                prompt=prompt,
                system_prompt=system_prompt,
                model=self.REASONING_MODEL,
                temperature=0.2,
                max_tokens=3000
            )
        except Exception as e:
            print(f"[AnalysisAgent] {self.REASONING_MODEL} í˜¸ì¶œ ì‹¤íŒ¨: {e}")

        # 2ì°¨ ì‹œë„: fallback to gpt-4o
        if not summary:
            try:
                print("[AnalysisAgent] Fallback: gpt-4o ì‚¬ìš©")
                summary = self._call_llm(
                    prompt=prompt,
                    system_prompt=system_prompt,
                    model="gpt-4o",
                    temperature=0.3,
                    max_tokens=2500
                )
            except Exception as e:
                print(f"[AnalysisAgent] gpt-4o í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                return {
                    "summary": f"ë¶„ì„ ìƒì„± ì˜¤ë¥˜: {e}",
                    "sources": all_sources
                }

        # 5. ì¶œì²˜ ì„¹ì…˜ í•­ìƒ ì¶”ê°€
        if all_sources and summary:
            # ê¸°ì¡´ ì¶œì²˜ ì„¹ì…˜ì´ ìˆìœ¼ë©´ ì œê±°
            if "### ì¶œì²˜" in summary:
                summary = summary.split("### ì¶œì²˜")[0].strip()

            summary += "\n\n---\n**ì¶œì²˜:**\n"
            for src in all_sources[:10]:
                summary += f"- [{src['idx']}] [{src['title']}]({src['url']})\n"

        return {
            "summary": summary or "ë¶„ì„ ê²°ê³¼ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
            "sources": all_sources
        }

    def run(self, context: AgentContext) -> Dict[str, Any]:
        """Agent ì‹¤í–‰"""
        question = context.query
        metadata = context.metadata or {}

        result = self.analyze(
            question=question,
            period=metadata.get("period", {"year": 2024, "quarter": 4}),
            region=metadata.get("region"),
            company=metadata.get("company", "LGE"),
            verbose=metadata.get("verbose", True)
        )

        return {
            "question": result.question,
            "kpi_change": {
                "kpi_name": result.kpi_change.kpi_name if result.kpi_change else None,
                "change_percent": result.kpi_change.change_percent if result.kpi_change else None,
                "previous_value": result.kpi_change.previous_value if result.kpi_change else None,
                "current_value": result.kpi_change.current_value if result.kpi_change else None,
            } if result.kpi_change else None,
            "hypotheses_count": len(result.hypotheses),
            "validated_count": len(result.validated_hypotheses),
            "sql_queries": result.sql_queries,
            "matched_events_count": sum(len(v) for v in result.matched_events.values()),
            "summary": result.summary,
            "sources": result.sources,
            "details": result.details
        }

    def _calculate_kpi_change(
        self,
        question: str,
        period: Dict,
        region: str = None
    ) -> Optional[KPIChange]:
        """
        ì§ˆë¬¸ì—ì„œ KPI ì¶”ì¶œ í›„ ë³€ë™ ê³„ì‚°

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            period: {"year": 2024, "quarter": 4}
            region: ì§€ì—­ ì½”ë“œ

        Returns:
            KPIChange ë˜ëŠ” None
        """
        # 1. ì§ˆë¬¸ì—ì„œ KPI ì¶”ì¶œ
        kpi_name = self._extract_kpi_from_question(question)
        kpi_info = self.KPI_PATTERNS.get(kpi_name)

        if not kpi_info:
            return None

        # 2. ê¸°ê°„ ê³„ì‚° (DATE í˜•ì‹: YYYY-MM-DD)
        year = period.get("year", 2024)
        quarter = period.get("quarter", 4)

        curr_start, curr_end = self._get_quarter_date_range(year, quarter)
        prev_start, prev_end = self._get_quarter_date_range(year - 1, quarter)  # ì „ë…„ ë™ê¸°

        # 3. ì§€ì—­ í•„í„° ìƒì„± (SUBSIDIARY_ID ê¸°ë°˜)
        region_filter = ""
        if region:
            subsidiaries = self.REGION_SUBSIDIARY_MAP.get(region.upper(), [])
            if not subsidiaries:
                subsidiaries = self.REGION_SUBSIDIARY_MAP.get(region, [])
            if subsidiaries:
                subs_str = ", ".join([f"'{s}'" for s in subsidiaries])
                region_filter = f"AND sh.SUBSIDIARY_ID IN ({subs_str})"

        # 4. SQL ì¿¼ë¦¬ ìƒì„± (í…œí”Œë¦¿ ì‚¬ìš©)
        sql_query = kpi_info["query_template"].format(
            prev_start=prev_start,
            prev_end=prev_end,
            curr_start=curr_start,
            curr_end=curr_end,
            region_filter=region_filter
        )

        # 5. SQL ì‹¤í–‰
        try:
            exec_result = self.hypothesis_validator.sql_executor.execute(sql_query)

            if not exec_result.success or exec_result.data is None:
                print(f"KPI ê³„ì‚° SQL ì‹¤í–‰ ì‹¤íŒ¨: {exec_result.error}")
                print(f"SQL: {sql_query}")
                return None

            data = exec_result.data.to_dict('records')

            prev_row = next((r for r in data if r.get('PERIOD') == 'Previous'), None)
            curr_row = next((r for r in data if r.get('PERIOD') == 'Current'), None)

            if not prev_row or not curr_row:
                print(f"KPI ë°ì´í„° ì—†ìŒ: prev={prev_row}, curr={curr_row}")
                return None

            prev_value = float(prev_row.get('TOTAL_VALUE', 0) or 0)
            curr_value = float(curr_row.get('TOTAL_VALUE', 0) or 0)

            if prev_value == 0:
                change_percent = 100.0 if curr_value > 0 else 0.0
            else:
                change_percent = ((curr_value - prev_value) / abs(prev_value)) * 100

            change_amount = curr_value - prev_value

            region_text = region.upper() if region else "ì „ì²´"
            period_info = f"{year}ë…„ Q{quarter} vs {year-1}ë…„ Q{quarter} ({region_text})"

            return KPIChange(
                kpi_name=kpi_name,
                previous_value=prev_value,
                current_value=curr_value,
                change_percent=round(change_percent, 1),
                change_amount=change_amount,
                period_info=period_info,
                region=region or "",
                sql_query=sql_query
            )

        except Exception as e:
            print(f"KPI ê³„ì‚° ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _extract_kpi_from_question(self, question: str) -> str:
        """ì§ˆë¬¸ì—ì„œ KPI ì¶”ì¶œ"""
        question_lower = question.lower()

        for kpi_name, info in self.KPI_PATTERNS.items():
            for keyword in info["keywords"]:
                if keyword in question_lower:
                    return kpi_name

        # ê¸°ë³¸ê°’: ë§¤ì¶œ
        return "ë§¤ì¶œ"

    def _get_quarter_range(self, year: int, quarter: int) -> tuple:
        """ë¶„ê¸° ì‹œì‘/ì¢…ë£Œ ì›” ê³„ì‚° (YEARMONTH í˜•ì‹)"""
        quarter_months = {
            1: ("01", "03"),
            2: ("04", "06"),
            3: ("07", "09"),
            4: ("10", "12")
        }
        start_month, end_month = quarter_months[quarter]
        return f"{year}-{start_month}", f"{year}-{end_month}"

    def _get_quarter_date_range(self, year: int, quarter: int) -> tuple:
        """ë¶„ê¸° ì‹œì‘/ì¢…ë£Œ ë‚ ì§œ ê³„ì‚° (DATE í˜•ì‹: YYYY-MM-DD)"""
        quarter_dates = {
            1: ("01-01", "03-31"),
            2: ("04-01", "06-30"),
            3: ("07-01", "09-30"),
            4: ("10-01", "12-31")
        }
        start_date, end_date = quarter_dates[quarter]
        return f"{year}-{start_date}", f"{year}-{end_date}"
